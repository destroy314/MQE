_wandb:
    value:
        cli_version: 0.18.5
        m: []
        python_version: 3.8.20
        t:
            "1":
                - 1
                - 55
            "2":
                - 1
                - 55
            "3":
                - 17
                - 23
                - 55
            "4": 3.8.20
            "5": 0.18.5
            "8":
                - 5
            "12": 0.18.5
            "13": linux-x86_64
alg_cfg:
    value:
        clip_param: 0.2
        desired_kl: 0.01
        entropy_coef: 0.01
        gamma: 0.99
        lam: 0.95
        learning_rate: 0.001
        max_grad_norm: 1
        num_learning_epochs: 5
        num_mini_batches: 4
        schedule: adaptive
        use_clipped_value_loss: true
        value_loss_coef: 1
policy_cfg:
    value:
        activation: elu
        actor_hidden_dims:
            - 128
            - 64
        critic_hidden_dims:
            - 128
            - 64
        init_noise_std: 1
        rnn_hidden_size: 64
        rnn_num_layers: 1
        rnn_type: lstm
runner_cfg:
    value:
        algorithm_class_name: PPO
        checkpoint: -1
        empirical_normalization: false
        experiment_name: test
        load_run: -1
        logger: wandb
        max_iterations: 1500
        neptune_project: legged_gym
        num_steps_per_env: 24
        policy_class_name: ActorCritic
        resume: false
        resume_path: null
        run_name: ""
        save_interval: 50
        wandb_entity: xander2077
        wandb_project: legged_gym
